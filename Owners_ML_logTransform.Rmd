---
title: "Owners_ML"
author: "Michael Lisa"
date: "12/3/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This code workbook will create the ML model for the number of owners.

# Set Up

```{r}
library(splitstackshape) # Used for stratified sampling
library(dplyr)
library(xgboost) # Load XGBoost
library(caret) # Load Caret
library(OptimalCutpoints) # Load optimal cutpoints
library(ggplot2) # Load ggplot2
library(xgboostExplainer) # Load XGboost Explainer
library(pROC) # Load proc
library(SHAPforxgboost) # Load shap for XGBoost
library(forecast)
library(Metrics)
source("a_insights_shap_functions.r")
library(parallel)
```


# Load and partition data:

```{r}
load("clean_data.rda")

#remove non-predictor/response columns
ML_data <- clean_data %>% select(-c(id, users_rated, rating_average, bgg_rank, complexity_average))

# year_published was read in as a character type for some reason
ML_data$year_published <- as.numeric(ML_data$year_published) 

ML_data$Log_owned_users <- log(ML_data$owned_users+1)

sum(is.na(clean_data))
sum(is.na(ML_data))
which(is.na(ML_data))

ML_data_no_NA <- na.omit(ML_data) # Remove NA rows

set.seed(42) # Set seed
# Perform stratified sampling
 split_data <- stratified(ML_data_no_NA, # Set dataset
                         group = "Log_owned_users", # Set variables to use for stratification
                         size = 0.2,  # Set size of test set
                         bothSets = TRUE ) # Return both training and test sets
 # Extract train data
 train_data <- split_data[[2]]
 # Extract test data
 test_data <- split_data[[1]]

# Check size
nrow(train_data)
nrow(test_data)
```
# Convert to DMatrix

```{r}
#colnames(train_data[, c(2:6, 8:199)]) #used to figure out names of columns to remove

# Create training matrix
dtrain <- xgb.DMatrix(data = as.matrix(train_data[, c(2:6, 8:199)]), 
                      label = train_data$Log_owned_users)
# Create test matrix
dtest <- xgb.DMatrix(data = as.matrix(test_data[, c(2:6, 8:199)]), 
                     label = test_data$Log_owned_users)
```


# Training Model

```{r}
#watchlist = list(train=dtrain, test=dtest)

set.seed(42)
bst_1 <- xgboost(data = dtrain, # Set training data
               
               nrounds = 100, # Set number of rounds
               
               verbose = 0, # 1 - Prints out fit
               #print_every_n = 20, # Prints out result every 20th iteration
               nthread = detectCores() - 2, # Setnumber of cores for parallel processing
              # watchlist=watchlist, #makes a watchlist to see how model works on test data
               objective = "reg:squarederror", # Set objective
               eval_metric = "rmse",
               eval_metric = "mae") # Set evaluation metric to use
```

```{r}
boost_preds <- exp(predict(bst_1, dtest))-1 # Create predictions for xgboost model

#check error
forecast::accuracy(boost_preds, test_data$owned_users)
rmse(test_data$owned_users, boost_preds)
```

# Tune number of trees

```{r}
set.seed(42)
bst <- xgb.cv(data = dtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
               eta = 0.1, # Set learning rate
              
               nrounds = 1000, # Set number of rounds
               early_stopping_rounds = 50, # Set number of rounds to stop at if there is no improvement
               
               verbose = 0, # 1 - Prints out fit
               nthread = detectCores() - 2, # Set number of parallel threads
               #print_every_n = 20, # Prints out result every 20th iteration
              
               objective = "reg:squarederror", # Set objective
               eval_metric = "rmse",
               eval_metric = "mae") # Set evaluation metric to use
```
Best iteration was 397, will go to 450 to make sure we get everything.

# Tune max_depth_vals and min_child_weight

```{r}
# Be Careful - This can take a very long time to run
max_depth_vals <- c(5, 7, 10, 15, 20, 25, 30) # Create vector of max depth values
min_child_weight <- c(3,5,7, 10, 15, 20, 25, 30) # Create vector of min child values

# Expand grid of parameter values
cv_params <- expand.grid(max_depth_vals, min_child_weight)
names(cv_params) <- c("max_depth", "min_child_weight")
# Create results vector
rmse_vec <- mae_vec <- rep(NA, nrow(cv_params)) 
# Loop through results
for(i in 1:nrow(cv_params)){
  set.seed(42)
  bst_tune <- xgb.cv(data = dtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.1, # Set learning rate
              max.depth = cv_params$max_depth[i], # Set max depth
              min_child_weight = cv_params$min_child_weight[i], # Set minimum number of samples in node to split
             
               
              nrounds = 450, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 0, # 1 - Prints out fit
              nthread = detectCores() - 2, # Set number of parallel threads
             # print_every_n = 20, # Prints out result every 20th iteration
               
              objective = "reg:squarederror", # Set objective
              eval_metric = "rmse", # Set evaluation metric to use
              eval_metric = "mae") # Set evaluation metric to use
  rmse_vec[i] <- bst_tune$evaluation_log$test_rmse_mean[bst_tune$best_ntreelimit]
  mae_vec[i] <- bst_tune$evaluation_log$test_mae_mean[bst_tune$best_ntreelimit]
  
}
```

# Plot heatmaps

```{r}
res_db <- cbind.data.frame(cv_params, rmse_vec)
names(res_db)[3] <- c("rmse") 
res_db$max_depth <- as.factor(res_db$max_depth) # Convert tree number to factor for plotting
res_db$min_child_weight <- as.factor(res_db$min_child_weight) # Convert node size to factor for plotting
# Print RMSE heatmap
g_2 <- ggplot(res_db, aes(y = max_depth, x = min_child_weight, fill = rmse)) + # set aesthetics
  geom_tile() + # Use geom_tile for heatmap
  theme_bw() + # Set theme
  scale_fill_gradient2(low = "blue", # Choose low color
                       mid = "white", # Choose mid color
                       high = "red", # Choose high color
                       midpoint =mean(res_db$rmse), # Choose mid point
                       space = "Lab", 
                       na.value ="grey", # Choose NA value
                       guide = "colourbar", # Set color bar
                       aesthetics = "fill") + # Select aesthetics to apply
  labs(title = "RMSE", x = "Minimum Child Weight", y = "Max Depth", fill = "Scale") # Set labels
g_2 # Generate plot
```

```{r}
res_db <- cbind.data.frame(cv_params, mae_vec)
names(res_db)[3] <- c("mae") 
res_db$max_depth <- as.factor(res_db$max_depth) # Convert tree number to factor for plotting
res_db$min_child_weight <- as.factor(res_db$min_child_weight) # Convert node size to factor for plotting
# Print RMSE heatmap
g_3 <- ggplot(res_db, aes(y = max_depth, x = min_child_weight, fill = mae)) + # set aesthetics
  geom_tile() + # Use geom_tile for heatmap
  theme_bw() + # Set theme
  scale_fill_gradient2(low = "blue", # Choose low color
                       mid = "white", # Choose mid color
                       high = "red", # Choose high color
                       midpoint =mean(res_db$mae), # Choose mid point
                       space = "Lab", 
                       na.value ="grey", # Choose NA value
                       guide = "colourbar", # Set color bar
                       aesthetics = "fill") + # Select aesthetics to apply
  labs(title = "MAE", x = "Minimum Child Weight", y = "Max Depth", fill = "Scale") # Set labels
g_3 # Generate plot
```

Go with child weight 10 and max depth 10.

# Tune gamma

```{r}
gamma_vals <- c(0, 0.05, 0.1, 0.15, 0.2) # Create vector of gamma values

# Be Careful - This can take a very long time to run
set.seed(42)
rmse_vec <- mae_vec <- rep(NA, length(gamma_vals))
for(i in 1:length(gamma_vals)){
  bst_tune <- xgb.cv(data = dtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.1, # Set learning rate
              max.depth = 10, # Set max depth
              min_child_weight = 10, # Set minimum number of samples in node to split
              gamma = gamma_vals[i], # Set minimum loss reduction for split

              
               
              nrounds = 450, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 0, # 1 - Prints out fit
              nthread = detectCores() - 2, # Set number of parallel threads
             # print_every_n = 20, # Prints out result every 20th iteration
               
              objective = "reg:squarederror", # Set objective
              eval_metric = "rmse", # Set evaluation metric to use
              eval_metric = "mae") # Set evaluation metric to use
  rmse_vec[i] <- bst_tune$evaluation_log$test_rmse_mean[bst_tune$best_ntreelimit]
  mae_vec[i] <- bst_tune$evaluation_log$test_mae_mean[bst_tune$best_ntreelimit]
  
}
```
```{r}
cbind.data.frame(gamma_vals, rmse_vec, mae_vec)
```

Gamma of 0.00 appears to be best.

# Check number of rounds again

```{r}
set.seed(42)
bst <- xgb.cv(data = dtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.1, # Set learning rate
              max.depth = 10, # Set max depth
              min_child_weight = 10, # Set minimum number of samples in node to split
              gamma = 0.00, # Set minimum loss reduction for split
             
               
              nrounds = 1000, # Set number of rounds
              early_stopping_rounds = 50, # Set number of rounds to stop at if there is no improvement
               
              verbose = 0, # 1 - Prints out fit
              nthread = detectCores() - 2, # Set number of parallel threads
              #print_every_n = 20, # Prints out result every 20th iteration
               
              objective = "reg:squarederror", # Set objective
              eval_metric = "rmse", # Set evaluation metric to use
              eval_metric = "mae") # Set evaluation metric to use
```

Best was round 172, switching to 250 rounds.

# Tune subsample and colsample_by_tree

```{r}
subsample <- c(0.6, 0.7, 0.8, 0.9, 1) # Create vector of subsample values
colsample_by_tree <- c(0.6, 0.7, 0.8, 0.9, 1) # Create vector of col sample values

# Expand grid of tuning parameters
cv_params <- expand.grid(subsample, colsample_by_tree)
names(cv_params) <- c("subsample", "colsample_by_tree")
# Create vectors to store results
rmse_vec <- mae_vec <- rep(NA, nrow(cv_params)) 
# Loop through parameter values
for(i in 1:nrow(cv_params)){
  set.seed(42)
  bst_tune <- xgb.cv(data = dtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.1, # Set learning rate
              max.depth = 10, # Set max depth
              min_child_weight = 10, # Set minimum number of samples in node to split
              gamma = 0.0, # Set minimum loss reduction for split
              subsample = cv_params$subsample[i], # Set proportion of training data to use in tree
              colsample_bytree = cv_params$colsample_by_tree[i], # Set number of variables to use in each tree
               
              nrounds = 250, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 0, # 1 - Prints out fit
              nthread = detectCores() - 2, # Set number of parallel threads
              #print_every_n = 20, # Prints out result every 20th iteration
               
              objective = "reg:squarederror", # Set objective
              eval_metric = "rmse", # Set evaluation metric to use
              eval_metric = "mae") # Set evaluation metric to use
  rmse_vec[i] <- bst_tune$evaluation_log$test_rmse_mean[bst_tune$best_ntreelimit]
  mae_vec[i] <- bst_tune$evaluation_log$test_mae_mean[bst_tune$best_ntreelimit]
  
}
```

 Visualize:
 
```{r}
res_db <- cbind.data.frame(cv_params, rmse_vec, mae_vec)
names(res_db)[3:4] <- c("rmse", "mae") 
res_db$subsample <- as.factor(res_db$subsample) # Convert tree number to factor for plotting
res_db$colsample_by_tree <- as.factor(res_db$colsample_by_tree) # Convert node size to factor for plotting
g_4 <- ggplot(res_db, aes(y = colsample_by_tree, x = subsample, fill = rmse)) + # set aesthetics
  geom_tile() + # Use geom_tile for heatmap
  theme_bw() + # Set theme
  scale_fill_gradient2(low = "blue", # Choose low color
    mid = "white", # Choose mid color
    high = "red", # Choose high color
    midpoint =mean(res_db$rmse), # Choose mid point
    space = "Lab", 
    na.value ="grey", # Choose NA value
    guide = "colourbar", # Set color bar
    aesthetics = "fill") + # Select aesthetics to apply
  labs(x = "Subsample", y = "Column Sample by Tree", fill = "RMSE") # Set labels
g_4 # Generate plot


g_5 <- ggplot(res_db, aes(y = colsample_by_tree, x = subsample, fill = mae)) + # set aesthetics
  geom_tile() + # Use geom_tile for heatmap
  theme_bw() + # Set theme
  scale_fill_gradient2(low = "blue", # Choose low color
    mid = "white", # Choose mid color
    high = "red", # Choose high color
    midpoint =mean(res_db$mae), # Choose mid point
    space = "Lab", 
    na.value ="grey", # Choose NA value
    guide = "colourbar", # Set color bar
    aesthetics = "fill") + # Select aesthetics to apply
  labs(x = "Subsample", y = "Column Sample by Tree", fill = "MAE") # Set labels
g_5 # Generate plot

res_db
```
 
Best is Subsample = 0.9 and Column Sapmle by Tree = 0.6
 
# Eta tuning

```{r}
set.seed(42)
bst_mod_1 <- xgb.cv(data = dtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.3, # Set learning rate
              max.depth = 10, # Set max depth
              min_child_weight = 10, # Set minimum number of samples in node to split
              gamma = 0.00, # Set minimum loss reduction for split
              subsample = 0.9, # Set proportion of training data to use in tree
              colsample_bytree =  0.6, # Set number of variables to use in each tree
               
              nrounds = 1000, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 0, # 1 - Prints out fit
              nthread = detectCores() - 2, # Set number of parallel threads
              #print_every_n = 20, # Prints out result every 20th iteration
              
              objective = "reg:squarederror", # Set objective
              eval_metric = "rmse", # Set evaluation metric to use
              eval_metric = "mae") # Set evaluation metric to use


set.seed(42)
bst_mod_2 <- xgb.cv(data = dtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.1, # Set learning rate
              max.depth = 10, # Set max depth
              min_child_weight = 10, # Set minimum number of samples in node to split
              gamma = 0.00, # Set minimum loss reduction for split
              subsample = 0.9, # Set proportion of training data to use in tree
              colsample_bytree =  0.6, # Set number of variables to use in each tree
               
              nrounds = 1000, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 0, # 1 - Prints out fit
              nthread = detectCores() - 2, # Set number of parallel threads
             # print_every_n = 20, # Prints out result every 20th iteration
              
              objective = "reg:squarederror", # Set objective
              eval_metric = "rmse", # Set evaluation metric to use
              eval_metric = "mae") # Set evaluation metric to use
set.seed(42)
bst_mod_3 <- xgb.cv(data = dtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.05, # Set learning rate
              max.depth = 10, # Set max depth
              min_child_weight = 10, # Set minimum number of samples in node to split
              gamma = 0.00, # Set minimum loss reduction for split
              subsample = 0.9, # Set proportion of training data to use in tree
              colsample_bytree =  0.6, # Set number of variables to use in each tree
               
              nrounds = 1000, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 0, # 1 - Prints out fit
              nthread = detectCores() - 2, # Set number of parallel threads
              #print_every_n = 20, # Prints out result every 20th iteration
              
              objective = "reg:squarederror", # Set objective
              eval_metric = "rmse", # Set evaluation metric to use
              eval_metric = "mae") # Set evaluation metric to use
set.seed(42)
bst_mod_4 <- xgb.cv(data = dtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.01, # Set learning rate
              max.depth = 10, # Set max depth
              min_child_weight = 10, # Set minimum number of samples in node to split
              gamma = 0.00, # Set minimum loss reduction for split
              subsample = 0.9, # Set proportion of training data to use in tree
              colsample_bytree =  0.6, # Set number of variables to use in each tree
               
              nrounds = 1000, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 0, # 1 - Prints out fit
              nthread = detectCores() - 2, # Set number of parallel threads
              #print_every_n = 20, # Prints out result every 20th iteration
              
              objective = "reg:squarederror", # Set objective
              eval_metric = "rmse", # Set evaluation metric to use
              eval_metric = "mae") # Set evaluation metric to use

set.seed(42)
bst_mod_5 <- xgb.cv(data = dtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.005, # Set learning rate
              max.depth = 10, # Set max depth
              min_child_weight = 10, # Set minimum number of samples in node to split
              gamma = 0.00, # Set minimum loss reduction for split
              subsample = 0.9, # Set proportion of training data to use in tree
              colsample_bytree =  0.6, # Set number of variables to use in each tree
               
              nrounds = 1000, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 0, # 1 - Prints out fit
              nthread = detectCores() - 2, # Set number of parallel threads
              #print_every_n = 20, # Prints out result every 20th iteration
              
              objective = "reg:squarederror", # Set objective
              eval_metric = "rmse", # Set evaluation metric to use
              eval_metric = "mae") # Set evaluation metric to use
```

plot etas:

```{r}
# Extract results for model with eta = 0.3
pd1 <- cbind.data.frame(bst_mod_1$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.3, nrow(bst_mod_1$evaluation_log)))
names(pd1)[3] <- "eta"
# Extract results for model with eta = 0.1
pd2 <- cbind.data.frame(bst_mod_2$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.1, nrow(bst_mod_2$evaluation_log)))
names(pd2)[3] <- "eta"
# Extract results for model with eta = 0.05
pd3 <- cbind.data.frame(bst_mod_3$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.05, nrow(bst_mod_3$evaluation_log)))
names(pd3)[3] <- "eta"
# Extract results for model with eta = 0.01
pd4 <- cbind.data.frame(bst_mod_4$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.01, nrow(bst_mod_4$evaluation_log)))
names(pd4)[3] <- "eta"
# Extract results for model with eta = 0.005
pd5 <- cbind.data.frame(bst_mod_5$evaluation_log[,c("iter", "test_rmse_mean")], rep(0.005, nrow(bst_mod_5$evaluation_log)))
names(pd5)[3] <- "eta"
# Join datasets
plot_data <- rbind.data.frame(pd1, pd2, pd3, pd4, pd5)
# Converty ETA to factor
plot_data$eta <- as.factor(plot_data$eta)
# Plot points
g_6 <- ggplot(plot_data, aes(x = iter, y = test_rmse_mean, color = eta))+
  geom_point(alpha = 0.5) +
  theme_bw() + # Set theme
  theme(panel.grid.major = element_blank(), # Remove grid
        panel.grid.minor = element_blank(), # Remove grid
        panel.border = element_blank(), # Remove grid
        panel.background = element_blank()) + # Remove grid 
  labs(x = "Number of Trees", title = "RMSE v Number of Trees",
       y = "RMSE", color = "Learning \n Rate")  # Set labels
g_6

# Plot lines
g_7 <- ggplot(plot_data, aes(x = iter, y = test_rmse_mean, color = eta))+
  geom_smooth(alpha = 0.5) +
  theme_bw() + # Set theme
  theme(panel.grid.major = element_blank(), # Remove grid
        panel.grid.minor = element_blank(), # Remove grid
        panel.border = element_blank(), # Remove grid
        panel.background = element_blank()) + # Remove grid 
  labs(x = "Number of Trees", title = "RMSE v Number of Trees",
       y = "RMSE", color = "Learning \n Rate")  # Set labels
g_7
```

Best eta appears to be 0.1

# Create final model!

```{r}
set.seed(42)
bst_final <- xgboost(data = dtrain, # Set training data
              
        
               
              eta = 0.1, # Set learning rate
              max.depth =  10, # Set max depth
              min_child_weight = 10, # Set minimum number of samples in node to split
              gamma = 0.00, # Set minimum loss reduction for split
              subsample =  0.9, # Set proportion of training data to use in tree
              colsample_bytree = 0.6, # Set number of variables to use in each tree
               
              nrounds = 250, # Set number of rounds
              early_stopping_rounds = 50, # Set number of rounds to stop at if there is no improvement
               
              verbose = 0, # 1 - Prints out fit
              nthread = detectCores() - 2, # Set number of parallel threads
              # print_every_n = 20, # Prints out result every 20th iteration
              
              objective = "reg:squarederror", # Set objective
              eval_metric = "rmse", # Set evaluation metric to use
              eval_metric = "mae") # Set evaluation metric to use
```

#Predict

```{r}
boost_preds_tuned <- exp(predict(bst_final, dtest))-1 # Create predictions for xgboost model

#check error
forecast::accuracy(boost_preds_tuned, test_data$owned_users)
rmse(test_data$owned_users, boost_preds_tuned)
```

Compared to pretune:

```{r}
boost_preds <- predict(bst_1, dtest) # Create predictions for xgboost model

#check error
forecast::accuracy(boost_preds, test_data$owned_users)
rmse(test_data$owned_users, boost_preds)
```

So we improved our accuracy in tuning by about 200 owners on average.

# Variable importance

```{r}
# Extract importance
imp_mat <- xgb.importance(model = bst_final)
# Plot importance (top 10 variables)
xgb.plot.importance(imp_mat, top_n = 20)
```

# Explainer

```{r}
explainer = buildExplainer(bst_final, dtrain, type="regression", base_score = 0.5, trees_idx = NULL) # Create explainer
pred.breakdown = explainPredictions(bst_final, explainer, dtest) # Breakdown predictions

# Create explainer for sample 1 
showWaterfall(bst_final, explainer, dtest, as.matrix(test_data[, c(2:6, 8:199)]) ,1, type = "regression", threshold = 0.07)
# Create explainer for sample 1441
showWaterfall(bst_final, explainer, dtest, as.matrix(test_data[, c(2:6, 8:199)]) ,1441, type = "regression", threshold = 0.07)
# Create explainer for sample 2000
showWaterfall(bst_final, explainer, dtest, as.matrix(test_data[, c(2:6, 8:199)]) ,2000, type = "regression", threshold = 0.07)

```

```{r}
x_vars <- as.matrix(train_data[, c(2:6, 8:199)])

# Calculate SHAP importance
shap_result <- shap.score.rank(xgb_model = bst_final, 
                X_train =x_vars,
                shap_approx = F)

shap_long = shap.prep(shap = shap_result,
                           X_train = x_vars, 
                           top_n = 20)


plot.shap.summary(data_long = shap_long)
```

